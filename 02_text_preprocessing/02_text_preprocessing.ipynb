{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self, text=\"test\"):\n",
    "        self.text = text\n",
    "\n",
    "    def lowercase(self):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        self.text = self.text.lower()\n",
    "        self.text = self.text.strip()\n",
    "        return self\n",
    "\n",
    "    def strip_html(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        soup = BeautifulSoup(self.text, \"lxml\")\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "\n",
    "    def remove_url(self):\n",
    "        \"\"\"Remove URL (http/https/www) or custom URL\"\"\"\n",
    "        self.text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", self.text)\n",
    "        self.text = re.sub(r\"pic.twitter.com\\S+\", \"\", self.text)  # custom for twitter\n",
    "        return self\n",
    "\n",
    "    def remove_email(self):\n",
    "        \"\"\"Remove email\"\"\"\n",
    "        self.text = re.sub(\"\\S*@\\S*\\s?\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_between_square_brackets(self):\n",
    "        \"\"\"Remove string beetwen square brackets []\"\"\"\n",
    "        self.text = re.sub(\"\\[[^]]*\\]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        \"\"\"Remove numbers\"\"\"\n",
    "        self.text = re.sub(\"[-+]?[0-9]+\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoji(self):\n",
    "        \"\"\"Remove emoji, e.g ðŸ˜œðŸ˜€ \"\"\"\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.text = emoji_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoticon(self):\n",
    "        \"\"\"Remove emoticon, e.g :-)\"\"\"\n",
    "        emoticon_pattern = re.compile(u\"(\" + u\"|\".join(k for k in EMOTICONS) + u\")\")\n",
    "        self.text = emoticon_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def convert_emoji(self):\n",
    "        \"\"\"Convert emoji to word\"\"\"\n",
    "        for emoji in UNICODE_EMO:\n",
    "            self.text = self.text.replace(\n",
    "                emoji,\n",
    "                \"_\".join(UNICODE_EMO[emoji].replace(\",\", \"\").replace(\":\", \"\").split()),\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def convert_emoticon(self):\n",
    "        \"\"\"Convert emoticon to word\"\"\"\n",
    "        for emoticon in EMOTICONS:\n",
    "            self.text = re.sub(\n",
    "                u\"(\" + emoticon + \")\",\n",
    "                \"_\".join(EMOTICONS[emoticon].replace(\",\", \"\").split()),\n",
    "                self.text,\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation\"\"\"\n",
    "        self.text = re.sub(r\"[^\\w\\s]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ascii character\"\"\"\n",
    "        self.text = (\n",
    "            unicodedata.normalize(\"NFKD\", self.text)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"utf-8\", \"ignore\")\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def normalize_word(self):\n",
    "        \"\"\"Normalize slang world\"\"\"\n",
    "        normal_word_path = pd.read_csv(\"../00_data/key_norm.csv\")\n",
    "\n",
    "        self.text = \" \".join(\n",
    "            [\n",
    "                normal_word_path[normal_word_path[\"singkat\"] == word][\"hasil\"].values[0]\n",
    "                if (normal_word_path[\"singkat\"] == word).any()\n",
    "                else word\n",
    "                for word in self.text.split()\n",
    "            ]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def stemming(self):\n",
    "        \"\"\"Stemming for Bahasa with Sastrawi\"\"\"\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        self.text = stemmer.stem(self.text)\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenize words\"\"\"\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def stopwords_removal(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        stopword = stopwords.words(\"indonesian\")\n",
    "        more_stopword = [\n",
    "            \"daring\",\n",
    "            \"online\",\n",
    "            \"pd\",\n",
    "        ]  # add more stopword to default corpus\n",
    "        stop_factory = stopword + more_stopword\n",
    "\n",
    "        clean_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stop_factory:\n",
    "                clean_words.append(word)\n",
    "        self.words = clean_words\n",
    "        return self\n",
    "\n",
    "    def join_words(self):\n",
    "        \"\"\"Jonin all words\"\"\"\n",
    "        self.words = \" \".join(self.words)\n",
    "        return self\n",
    "\n",
    "    def do_all(self, text):\n",
    "        \"\"\"Do all text preprocessing process\"\"\"  # or custom process\n",
    "        self.text = text\n",
    "        self = self.lowercase()\n",
    "        self = self.strip_html()\n",
    "        self = self.remove_url()\n",
    "        self = self.remove_email()\n",
    "        self = self.remove_between_square_brackets()\n",
    "        self = self.remove_numbers()\n",
    "        self = self.remove_emoticon()\n",
    "        self = self.remove_emoji()\n",
    "        self = self.convert_emoticon()\n",
    "        self = self.convert_emoji()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.normalize_word()\n",
    "        self = self.stemming()\n",
    "        self = self.tokenize()\n",
    "        self = self.stopwords_removal()\n",
    "        self = self.join_words()\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tickets</th>\n",
       "      <th>OPD</th>\n",
       "      <th>id</th>\n",
       "      <th>tickets_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mohon info agar KK saya dan E KTP saya agar se...</td>\n",
       "      <td>DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya warga Kelurahan Karangbesuki yang sedang ...</td>\n",
       "      <td>DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...</td>\n",
       "      <td>1</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Selamat sore. Saya warga kel. Pisang candi, ke...</td>\n",
       "      <td>DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...</td>\n",
       "      <td>1</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mohon informasi apakah blanko e ktp sudah ada?...</td>\n",
       "      <td>DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saya mau merubah akte saya yang salah penulisa...</td>\n",
       "      <td>DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tickets  \\\n",
       "0  Mohon info agar KK saya dan E KTP saya agar se...   \n",
       "1  Saya warga Kelurahan Karangbesuki yang sedang ...   \n",
       "2  Selamat sore. Saya warga kel. Pisang candi, ke...   \n",
       "3  mohon informasi apakah blanko e ktp sudah ada?...   \n",
       "4  saya mau merubah akte saya yang salah penulisa...   \n",
       "\n",
       "                                                 OPD  id  tickets_length  \n",
       "0  DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...   1             153  \n",
       "1  DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...   1             625  \n",
       "2  DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...   1             877  \n",
       "3  DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...   1             136  \n",
       "4  DINAS KEPENDUDUKAN DAN PENCATATAN SIPIL KOTA M...   1             154  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../01_exploratory_data_analysis/01_pickle/01_data_training.pickle'\n",
    "\n",
    "with open(data_path, 'rb') as data_training:\n",
    "    data = pickle.load(data_training)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing success !\n",
      "Elapsed time: 9.71346378326416 seconds\n",
      "\n",
      "Finish :)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "tp = TextPreprocessing() # load module text preprocessing\n",
    "\n",
    "def dask_this(data):\n",
    "    data['clean_tickets'] = data['tickets'].apply(tp.do_all)\n",
    "    return data\n",
    "\n",
    "ddata = dd.from_pandas(data, npartitions=10)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    data = ddata.map_partitions(dask_this).compute(scheduler='processes', num_workers=10)\n",
    "except:\n",
    "    print('Text preprocessing failed !')\n",
    "else:\n",
    "    data.to_csv('../00_data/clean_data_training.csv', encoding='utf-8')\n",
    "    print('Text preprocessing success !')\n",
    "    print('Elapsed time:', time.time() - start_time, 'seconds')\n",
    "finally:\n",
    "    print('\\nFinish :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['clean_tickets', 'OPD']\n",
    "data = data[columns]\n",
    "\n",
    "with open('02_pickle/02_clean_data.pickle', 'wb') as output:\n",
    "    pickle.dump(data, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
